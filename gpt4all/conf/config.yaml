output_dir: "tuned-model"


dataset:
  wandb : 1
  # either 'bittensor', a local path, or one from huggingface
  name: "/opentensor/opentensor-validator/runs/kltiefxf"
  config_name: "/opentensor/opentensor-validator/runs/kltiefxf" # necessary for huggingface datasets
  block_size: 5 # if null, defaults to bittensor's validator sequence length.

  data_dir: "train_data" #specify any folder name you want
  shuffle: true

  
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false # only really necessary when loading a local .txt file

model:
  name: nomic-ai/gpt4all-j
  config_name: nomic-ai/gpt4all-j

tokenizer:
  name: nomic-ai/gpt4all-j
  use_fast: true
  preprocessing_num_workers: null
  pad_token: "[PAD]"

training:

  seed: 30
  # if null these both default to bittensor's validator batch size
  train_batch_size: 2
  eval_batch_size: 2

  learning_rate: 2e-5
  weight_decay: 0.0
  num_epochs: 1
  max_train_steps: null
  gradient_accumulation_steps: 4
  lr_scheduler: "constant" # ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
  lr_warmup_steps: 0
  eval_every: epoch

  checkpoint:
    resume_from_checkpoint: 0 # integer representing which checkpoint to load from, or <= 0 to not
    checkpointing_steps: epoch
    every_n_steps: null
    
  hub:
    push_to_hub: false
    model_id: null
    token: null

load_dir: null  

tracking:
  enabled: false
  report_to: "all"
  
load_best_model: true
